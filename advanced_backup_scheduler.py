#!/usr/bin/env python
"""
ê³ ê¸‰ PostgreSQL ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
- AWS S3 ë˜ëŠ” Google Cloud Storageì— ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ
- ì´ë©”ì¼/ìŠ¬ë™ ì•Œë¦¼
- ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
"""
import os
import sys
import subprocess
import datetime
import hashlib
import gzip
from pathlib import Path

# Django ì„¤ì • ì´ˆê¸°í™”
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'sales_project.settings')

import django
django.setup()

class BackupManager:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_filename = f"railway_sales_backup_{self.timestamp}"
        self.temp_dir = Path("/tmp")
        
    def create_postgres_dump(self):
        """PostgreSQL ë¤í”„ ìƒì„± (ì••ì¶• í¬í•¨)"""
        try:
            database_url = os.environ.get('DATABASE_PUBLIC_URL')
            if not database_url:
                raise Exception("DATABASE_PUBLIC_URL í™˜ê²½ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë¤í”„ íŒŒì¼ ê²½ë¡œ
            sql_file = self.temp_dir / f"{self.backup_filename}.sql"
            compressed_file = self.temp_dir / f"{self.backup_filename}.sql.gz"
            
            print(f"ğŸ”„ PostgreSQL ë¤í”„ ìƒì„± ì‹œì‘...")
            
            # pg_dump ì‹¤í–‰
            dump_command = [
                "pg_dump",
                database_url,
                "--verbose",
                "--no-password",
                "--format=plain",  # ì¼ë°˜ SQL í˜•íƒœ
                "--file", str(sql_file)
            ]
            
            result = subprocess.run(
                dump_command,
                capture_output=True,
                text=True,
                timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode != 0:
                raise Exception(f"pg_dump ì‹¤íŒ¨: {result.stderr}")
            
            # íŒŒì¼ ì••ì¶•
            print(f"ğŸ—œï¸ ë°±ì—… íŒŒì¼ ì••ì¶• ì¤‘...")
            with open(sql_file, 'rb') as f_in:
                with gzip.open(compressed_file, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            # ì›ë³¸ SQL íŒŒì¼ ì‚­ì œ
            sql_file.unlink()
            
            # íŒŒì¼ í¬ê¸° ë° ì²´í¬ì„¬ ê³„ì‚°
            file_size = compressed_file.stat().st_size
            checksum = self.calculate_checksum(compressed_file)
            
            print(f"âœ… ë¤í”„ ìƒì„± ì™„ë£Œ:")
            print(f"   ğŸ“ íŒŒì¼: {compressed_file.name}")
            print(f"   ğŸ“ í¬ê¸°: {file_size / 1024 / 1024:.2f} MB")
            print(f"   ğŸ” ì²´í¬ì„¬: {checksum}")
            
            return compressed_file, checksum, file_size
            
        except subprocess.TimeoutExpired:
            raise Exception("ë¤í”„ ìƒì„± íƒ€ì„ì•„ì›ƒ (10ë¶„ ì´ˆê³¼)")
        except Exception as e:
            raise Exception(f"ë¤í”„ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def calculate_checksum(self, file_path):
        """íŒŒì¼ MD5 ì²´í¬ì„¬ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def upload_to_aws_s3(self, file_path, checksum):
        """AWS S3ì— ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            import boto3
            from botocore.exceptions import ClientError
            
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ AWS ì„¤ì • ì½ê¸°
            aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')
            aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
            bucket_name = os.environ.get('AWS_S3_BACKUP_BUCKET')
            
            if not all([aws_access_key, aws_secret_key, bucket_name]):
                print("âš ï¸ AWS S3 ì„¤ì •ì´ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            
            print(f"â˜ï¸ AWS S3ì— ì—…ë¡œë“œ ì¤‘: {bucket_name}")
            
            s3_client = boto3.client(
                's3',
                aws_access_key_id=aws_access_key,
                aws_secret_access_key=aws_secret_key
            )
            
            # S3 ê°ì²´ í‚¤ (ê²½ë¡œ)
            s3_key = f"sales-note-backups/{file_path.name}"
            
            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì—…ë¡œë“œ
            s3_client.upload_file(
                str(file_path),
                bucket_name,
                s3_key,
                ExtraArgs={
                    'Metadata': {
                        'checksum': checksum,
                        'backup_date': self.timestamp,
                        'source': 'railway-postgres'
                    }
                }
            )
            
            print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket_name}/{s3_key}")
            return True
            
        except ImportError:
            print("âš ï¸ boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ S3 ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        except ClientError as e:
            print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def upload_to_google_cloud(self, file_path, checksum):
        """Google Cloud Storageì— ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            from google.cloud import storage
            
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ GCS ì„¤ì • ì½ê¸°
            bucket_name = os.environ.get('GCS_BACKUP_BUCKET')
            credentials_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')
            
            if not bucket_name:
                print("âš ï¸ GCS ì„¤ì •ì´ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            
            print(f"â˜ï¸ Google Cloud Storageì— ì—…ë¡œë“œ ì¤‘: {bucket_name}")
            
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            
            # ë¸”ë¡­ ì´ë¦„ (ê²½ë¡œ)
            blob_name = f"sales-note-backups/{file_path.name}"
            blob = bucket.blob(blob_name)
            
            # ë©”íƒ€ë°ì´í„° ì„¤ì •
            blob.metadata = {
                'checksum': checksum,
                'backup_date': self.timestamp,
                'source': 'railway-postgres'
            }
            
            # ì—…ë¡œë“œ
            blob.upload_from_filename(str(file_path))
            
            print(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: gs://{bucket_name}/{blob_name}")
            return True
            
        except ImportError:
            print("âš ï¸ google-cloud-storageê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ GCS ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ GCS ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def send_email_notification(self, success, message, file_info=None):
        """ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡"""
        try:
            import smtplib
            from email.mime.text import MIMEText
            from email.mime.multipart import MIMEMultipart
            
            # ì´ë©”ì¼ ì„¤ì •
            smtp_server = os.environ.get('SMTP_SERVER')
            smtp_port = int(os.environ.get('SMTP_PORT', '587'))
            smtp_username = os.environ.get('SMTP_USERNAME')
            smtp_password = os.environ.get('SMTP_PASSWORD')
            admin_email = os.environ.get('ADMIN_EMAIL')
            
            if not all([smtp_server, smtp_username, smtp_password, admin_email]):
                print("âš ï¸ ì´ë©”ì¼ ì„¤ì •ì´ ì—†ì–´ ì•Œë¦¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return False
            
            # ì´ë©”ì¼ ë‚´ìš© êµ¬ì„±
            msg = MIMEMultipart()
            msg['From'] = smtp_username
            msg['To'] = admin_email
            msg['Subject'] = f"Sales Note DB ë°±ì—… {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'} - {datetime.datetime.now().strftime('%Y-%m-%d')}"
            
            body = f"""
Sales Note PostgreSQL ë°±ì—… ê²°ê³¼:

ìƒíƒœ: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}
ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ë©”ì‹œì§€: {message}
"""
            
            if file_info and success:
                body += f"""
ë°±ì—… íŒŒì¼ ì •ë³´:
- íŒŒì¼ëª…: {file_info['filename']}
- í¬ê¸°: {file_info['size']:.2f} MB
- ì²´í¬ì„¬: {file_info['checksum']}
"""
            
            msg.attach(MIMEText(body, 'plain'))
            
            # ì´ë©”ì¼ ì „ì†¡
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(smtp_username, smtp_password)
            server.send_message(msg)
            server.quit()
            
            print("ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ì´ë©”ì¼ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup_old_files(self, keep_days=7):
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.datetime.now() - datetime.timedelta(days=keep_days)
            deleted_count = 0
            
            for backup_file in self.temp_dir.glob("railway_sales_backup_*.sql.gz"):
                file_time = datetime.datetime.fromtimestamp(backup_file.stat().st_mtime)
                
                if file_time < cutoff_date:
                    backup_file.unlink()
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_file.name}")
            
            if deleted_count > 0:
                print(f"âœ… {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë¡œì»¬ ë°±ì—… ì‚­ì œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def run_backup(self):
        """ì „ì²´ ë°±ì—… í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 70)
        print(f"ğŸš€ Sales Note PostgreSQL ê³ ê¸‰ ë°±ì—… ì‹œìŠ¤í…œ")
        print(f"â° ì‹œì‘ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        
        try:
            # 1. ë¤í”„ ìƒì„±
            backup_file, checksum, file_size = self.create_postgres_dump()
            
            file_info = {
                'filename': backup_file.name,
                'size': file_size / 1024 / 1024,  # MB
                'checksum': checksum
            }
            
            # 2. í´ë¼ìš°ë“œ ì—…ë¡œë“œ
            aws_success = self.upload_to_aws_s3(backup_file, checksum)
            gcs_success = self.upload_to_google_cloud(backup_file, checksum)
            
            # 3. ë¡œì»¬ íŒŒì¼ ì •ë¦¬
            self.cleanup_old_files()
            
            # 4. ë°±ì—… íŒŒì¼ ì‚­ì œ (í´ë¼ìš°ë“œ ì—…ë¡œë“œ ì„±ê³µ ì‹œ)
            if aws_success or gcs_success:
                backup_file.unlink()
                print("ğŸ—‘ï¸ ë¡œì»¬ ë°±ì—… íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
            
            # 5. ì„±ê³µ ì•Œë¦¼
            message = f"ë°±ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (í¬ê¸°: {file_info['size']:.2f} MB)"
            print(f"ğŸ‰ {message}")
            
            self.send_email_notification(True, message, file_info)
            
            return True
            
        except Exception as e:
            error_message = f"ë°±ì—… ì‹¤íŒ¨: {e}"
            print(f"ğŸ’¥ {error_message}")
            
            self.send_email_notification(False, error_message)
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    backup_manager = BackupManager()
    
    try:
        success = backup_manager.run_backup()
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ë°±ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

if __name__ == "__main__":
    main()
